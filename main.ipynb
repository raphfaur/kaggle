{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "gpd.options.io_engine = \"pyogrio\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/foxy/Documents/CS/Machine Learning/.env/lib/python3.11/site-packages/pyogrio/raw.py:194: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  result = ogr_read(\n",
      "/Users/foxy/Documents/CS/Machine Learning/.env/lib/python3.11/site-packages/pyogrio/raw.py:194: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  result = ogr_read(\n"
     ]
    }
   ],
   "source": [
    "train_df = gpd.read_file(\"./data/train.geojson\", index_col=0);\n",
    "test_df = gpd.read_file(\"./data/test.geojson\", index_col=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296146, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shapely\n",
    "\n",
    "# Replace NaN colors with mean \n",
    "cols = [col for col in train_df if col.startswith(\"img\")]\n",
    "test_df[cols] = test_df[cols].fillna(test_df[cols].astype(float).mean())\n",
    "clean_df = train_df\n",
    "clean_df[cols] = clean_df[cols].fillna(clean_df[cols].astype(float).mean())\n",
    "\n",
    "# Delete None change_status\n",
    "cols = [col for col in clean_df if col.startswith(\"change_status\")]\n",
    "\n",
    "cols = [col for col in clean_df if col.startswith(\"date\")]\n",
    "rows = test_df[cols].apply(lambda x : x.isnull() != False ).any(axis = \"columns\")\n",
    "test_df.loc[rows, cols] = '1-01-2001'\n",
    "\n",
    "rows = clean_df[cols].apply(lambda x : x.isnull() != False ).any(axis = \"columns\")\n",
    "clean_df.loc[rows, cols] = '1-01-2001'\n",
    "\n",
    "\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Geo data processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120526, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "geo = clean_df[\"geometry\"];\n",
    "geo_test = test_df[\"geometry\"];\n",
    "\n",
    "\n",
    "#Get area fo training\n",
    "area_data = geo.apply(lambda x : x.area)\n",
    "area_data = np.asarray(area_data)\n",
    "area_data = np.reshape(area_data, (-1, 1))\n",
    "n_area_data = (area_data - np.mean(area_data, axis=0))/np.std(area_data, axis=0) \n",
    "\n",
    "length_data = geo.apply(lambda x : x.length)\n",
    "length_data = np.asarray(length_data)\n",
    "length_data = np.reshape(length_data, (-1, 1))\n",
    "z_length_data = (length_data - np.mean(length_data, axis=0))/np.std(length_data, axis=0) \n",
    "\n",
    "\n",
    "# Get area for submission\n",
    "area_data_test = geo_test.apply(lambda x : x.area)\n",
    "area_data_test = np.asarray(area_data_test)\n",
    "area_data_test = np.reshape(area_data_test, (-1, 1))\n",
    "# min-max normalization\n",
    "n_area_data_test = (area_data_test - np.mean(area_data, axis=0))/np.std(area_data, axis=0) \n",
    "\n",
    "length_data_test = geo_test.apply(lambda x : x.length)\n",
    "length_data_test = np.asarray(length_data_test)\n",
    "length_data_test = np.reshape(length_data_test, (-1, 1))\n",
    "# z normalization\n",
    "z_length_data_test = (length_data_test - np.mean(length_data, axis=0))/np.std(length_data, axis=0) \n",
    "print(n_area_data_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Geotypes processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotypes = set()\n",
    "\n",
    "geography_types_test = test_df[\"geography_type\"]\n",
    "geography_types_array_test = np.asarray(test_df[\"geography_type\"])\n",
    "\n",
    "geography_types = clean_df[\"geography_type\"]\n",
    "geography_types_array = np.asarray(clean_df[\"geography_type\"])\n",
    "\n",
    "# List all categories\n",
    "def count(x) :\n",
    "    l = x.split(',')\n",
    "    for m in l :\n",
    "        if m == 'A' or m =='N' :\n",
    "            break\n",
    "        geotypes.add(m)\n",
    "    return x\n",
    "\n",
    "count = np.vectorize(count)\n",
    "count(geography_types_array)\n",
    "\n",
    "geotypes = list(geotypes)\n",
    "size = len(geotypes)\n",
    "\n",
    "geotypes_data = np.ndarray((geography_types_array.shape[0], size))\n",
    "geotypes_data_test = np.ndarray((geography_types_array_test.shape[0], size))\n",
    "\n",
    "# Compute for training\n",
    "for i,t in enumerate(geography_types_array) :\n",
    "    for m in t.split(',') :\n",
    "        # N/A value leads to the null vector\n",
    "        if m == 'A' or m =='N' :\n",
    "            break\n",
    "        geotypes_data[i,geotypes.index(m)] = 1\n",
    "\n",
    "# Compute for submission\n",
    "for i,t in enumerate(geography_types_array_test) :\n",
    "    for m in t.split(',') :\n",
    "        # N/A value leads to the null vector\n",
    "        if m == 'A' or m =='N' :\n",
    "            break\n",
    "        geotypes_data_test[i,geotypes.index(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Urbantypes processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Get all types\n",
    "urbantypes = set()\n",
    "\n",
    "urban_type_test = test_df[\"urban_type\"]\n",
    "urban_types_array_test = np.asarray(test_df[\"urban_type\"])\n",
    "\n",
    "urban_types = clean_df[\"urban_type\"]\n",
    "urban_types_array = np.asarray(clean_df[\"urban_type\"])\n",
    "\n",
    "# List all categories\n",
    "def count(x) :\n",
    "    l = x.split(',')\n",
    "    for m in l :\n",
    "        if m != 'A' and m != 'N' :\n",
    "            urbantypes.add(m)\n",
    "    return x\n",
    "\n",
    "count = np.vectorize(count)\n",
    "count(urban_types_array)\n",
    "urbantypes = list(urbantypes)\n",
    "size = len(urbantypes)\n",
    "\n",
    "# Compute for training set\n",
    "urbantypes_data = np.ndarray((urban_types_array.shape[0], size))\n",
    "for i,t in enumerate(urban_types_array) :\n",
    "    for m in t.split(',') :\n",
    "        if m == 'A' or m == 'N' :\n",
    "            break\n",
    "        urbantypes_data[i,urbantypes.index(m)] = 1\n",
    "\n",
    "# Compute for submission set\n",
    "urbantypes_data_test = np.ndarray((urban_types_array_test.shape[0], size))\n",
    "for i,t in enumerate(urban_types_array_test) :\n",
    "    for m in t.split(',') :\n",
    "        if m == 'A' or m == 'N' :\n",
    "            break\n",
    "        urbantypes_data_test[i,urbantypes.index(m)] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Status preprocessing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "columns = [col for col in clean_df if col.startswith(\"change_status\")]\n",
    "status = clean_df[columns]\n",
    "status_test = test_df[columns]\n",
    "\n",
    "status = np.asarray(status)\n",
    "status_test = np.asarray(status_test)\n",
    "\n",
    "# Fit OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(status)\n",
    "\n",
    "# Encode training\n",
    "status_data = encoder.transform(status).toarray()\n",
    "\n",
    "# Encode submission\n",
    "status_data_test = encoder.transform(status_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Date processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "columns = [col for col in clean_df if col.startswith(\"date\")]\n",
    "dates = clean_df[columns]\n",
    "dates_test = test_df[columns]\n",
    "\n",
    "\n",
    "for i in range(5) :\n",
    "    dates[[f'day{i}',f'month{i}',f'year{i}']] = dates[f'date{i}'].str.split('-', expand=True).astype(int)\n",
    "    dates_test[[f'day{i}',f'month{i}',f'year{i}']] = dates_test[f'date{i}'].str.split('-', expand=True).astype(int)\n",
    "\n",
    "c_days = [col for col in dates if col.startswith(\"day\")]\n",
    "c_months = [col for col in dates if col.startswith(\"month\")]\n",
    "c_year = [col for col in dates if col.startswith(\"year\")]\n",
    "\n",
    "time = pd.concat([dates[c_days],dates[c_months], dates[c_year]], axis=1)\n",
    "time = pd.concat([dates_test[c_days],dates_test[c_months], dates_test[c_year]], axis=1)\n",
    "\n",
    "# Compute date in days\n",
    "def time(row, j) :\n",
    "    return row[f'year{j}'] * 365 + row[f'month{j}'] * 30 + row[f'day{j}']  * 1\n",
    "\n",
    "for i in range(5) :\n",
    "    dates[f'time{i}'] = dates.apply(time, axis=1, j=i).astype(int)\n",
    "    dates_test[f'time{i}'] = dates_test.apply(time, axis=1, j=i).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dates beacause they are not\n",
    "time_array = np.stack(dates[[f'time{i}' for i in range(5)]].astype(int).apply(np.sort, axis=1).to_numpy())\n",
    "time_array_test = np.stack(dates_test[[f'time{i}' for i in range(5)]].astype(int).apply(np.sort, axis=1).to_numpy())\n",
    "\n",
    "elapsed = np.ndarray((time_array.shape[0],4))\n",
    "elapsed_test = np.ndarray((time_array_test.shape[0],4))\n",
    "\n",
    "# Compute elapsed time\n",
    "for i in range(4) :\n",
    "    elapsed[:,i] = time_array[:,i+1] - time_array[:,i]\n",
    "    elapsed_test[:,i] = time_array_test[:,i+1] - time_array_test[:,i]\n",
    "\n",
    "# min max normalization\n",
    "minmax_elapsed = (elapsed - np.min(elapsed, axis=0)) / (np.max(elapsed, axis=0)- np.min(elapsed, axis=0))\n",
    "minmax_elapsed_test = (elapsed_test - np.min(elapsed, axis=0)) / (np.max(elapsed, axis=0) - np.min(elapsed, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Color data processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 30)\n"
     ]
    }
   ],
   "source": [
    "# Selects every column starting with \"img\"\n",
    "# We could have separated std deviation and mean ?\n",
    "cols = [col for col in clean_df if \"img\" in col]\n",
    "\n",
    "# Build an array from the data\n",
    "color = np.asarray(clean_df[cols])\n",
    "mean = np.mean(color, axis=0)\n",
    "std = np.std(color, axis=0)\n",
    "ma = np.max(color, axis=0)\n",
    "mi = np.min(color, axis=0)\n",
    "\n",
    "# Normalize data\n",
    "color_test = test_df[cols]\n",
    "mean_test = np.mean(color_test, axis=0)\n",
    "std_test = np.std(color_test, axis=0)\n",
    "ma_test = np.max(color_test, axis=0)\n",
    "mi_test = np.min(color_test, axis=0)\n",
    "\n",
    "minimax_color = (color-mi)/(ma-mi)\n",
    "z_color = (color-mean)/std\n",
    "z_color_test = (color_test - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Training <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Create training set\n",
    "labels = [\"Demolition\", \"Road\", \"Residential\", \"Commercial\", \"Industrial\", \"Mega Projects\"]\n",
    "X = np.concatenate((area_data, geotypes_data, urbantypes_data, status_data, color, length_data), axis=1)\n",
    "\n",
    "data_labels = clean_df[\"change_type\"]\n",
    "data_labels = data_labels.apply(lambda x : labels.index(x))\n",
    "\n",
    "# Vector data set\n",
    "Y_vector = utils.to_categorical(y,num_classes=6)\n",
    "X_train, X_test, Y_vector_train, Y_vector_test = train_test_split(X, Y_vector, test_size=0.1, random_state=42)\n",
    "\n",
    "# Class data set\n",
    "Y_class = np.asarray(y)\n",
    "\n",
    "X_train, X_test, Y_class_train, Y_class_test = train_test_split(X, Y_class, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import models, layers\n",
    "from keras import backend as K\n",
    "\n",
    "dataset_size = X_train.shape[0]\n",
    "features = X_train.shape[1]\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.build((dataset_size,features))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer='sgd', metrics=['accuracy', f1_m])\n",
    "model.fit(X_train, Y_vector_train,batch_size=30, validation_split=0.2, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Training RandomForest <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7238899206483201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7238899206483201"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=40)\n",
    "clf.fit(X_train, Y_class_train)\n",
    "# Compute training accuracy to check overfitting\n",
    "print(clf.score(X_train[:5000],Y_class_train[:5000]))\n",
    "# Compute validation accuracy\n",
    "print(clf.score(X_test,Y_class_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "# Compute f1 score\n",
    "sklearn.metrics.f1_score(y_pred, Y_class_test,  average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Submission file <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = np.concatenate((area_data_test, geotypes_data_test, urbantypes_data_test, status_data_test, color_test, length_data_test), axis=1)\n",
    "Y_submission = clf.predict(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(Y_submission, columns=['change_type'])\n",
    "s.index.name = \"Id\"\n",
    "s.to_csv('test7.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
