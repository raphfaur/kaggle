{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "gpd.options.io_engine = \"pyogrio\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/foxy/Documents/CS/Machine Learning/.env/lib/python3.11/site-packages/pyogrio/raw.py:194: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  result = ogr_read(\n",
      "/Users/foxy/Documents/CS/Machine Learning/.env/lib/python3.11/site-packages/pyogrio/raw.py:194: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  result = ogr_read(\n"
     ]
    }
   ],
   "source": [
    "train_df = gpd.read_file(\"./data/train.geojson\", index_col=0);\n",
    "test_df = gpd.read_file(\"./data/test.geojson\", index_col=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296146, 45)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shapely\n",
    "\n",
    "# Replace NaN colors with mean \n",
    "cols = [col for col in train_df if col.startswith(\"img\")]\n",
    "test_df[cols] = test_df[cols].fillna(test_df[cols].astype(float).mean())\n",
    "clean_df = train_df\n",
    "clean_df[cols] = clean_df[cols].fillna(clean_df[cols].astype(float).mean())\n",
    "\n",
    "# Delete None change_status\n",
    "cols = [col for col in clean_df if col.startswith(\"change_status\")]\n",
    "# rows = clean_df[cols].apply(lambda x : x.isnull() == False ).all(axis = \"columns\")\n",
    "# clean_df = clean_df[rows]\n",
    "\n",
    "cols = [col for col in clean_df if col.startswith(\"date\")]\n",
    "rows = test_df[cols].apply(lambda x : x.isnull() != False ).any(axis = \"columns\")\n",
    "test_df.loc[rows, cols] = '1-01-2001'\n",
    "\n",
    "rows = clean_df[cols].apply(lambda x : x.isnull() != False ).any(axis = \"columns\")\n",
    "clean_df.loc[rows, cols] = '1-01-2001'\n",
    "\n",
    "\n",
    "# Delete polygon with more that 4 points\n",
    "# rows = clean_df[\"geometry\"].apply(lambda x : shapely.get_num_points(x.boundary))\n",
    "# rows = rows == 5\n",
    "# clean_df = clean_df[rows]\n",
    "# print(rows.shape)\n",
    "\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Geo data processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to compute main angle of a polygon in degree\n",
    "\n",
    "from shapely import LineString\n",
    "\n",
    "def segments(curve):\n",
    "    return list(map(LineString, zip(curve.coords[:-1], curve.coords[1:])))\n",
    "\n",
    "def main_angle(polygon) :\n",
    "    curve = polygon.boundary\n",
    "    seg = segments(curve)\n",
    "    max_length = 0\n",
    "    angle = 0\n",
    "    for s in seg :\n",
    "        coords = s.coords.xy\n",
    "        real = coords[0][1] - coords[0][0]\n",
    "        img = coords[1][1] - coords[1][0]\n",
    "        z = real + img*1j\n",
    "        l = np.sqrt(real**2 + img**2)\n",
    "        if l > max_length :\n",
    "            max_length = l\n",
    "            angle = np.angle(z, deg = True)\n",
    "    return angle % 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120526, 1)\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "def coord_lister(geom):\n",
    "    coords = list(geom.exterior.coords)\n",
    "    return list(sum(coords, ()))\n",
    "\n",
    "def shape(geom) :\n",
    "    segments = list(map(LineString, zip(curve.coords[:-1], curve.coords[1:])))\n",
    "\n",
    "geo = clean_df[\"geometry\"];\n",
    "geo_test = test_df[\"geometry\"];\n",
    "\n",
    "\n",
    "#Get area fo training\n",
    "area_data = geo.apply(lambda x : x.area)\n",
    "area_data = np.asarray(area_data)\n",
    "area_data = np.reshape(area_data, (-1, 1))\n",
    "n_area_data = (area_data - np.mean(area_data, axis=0))/np.std(area_data, axis=0) \n",
    "\n",
    "length_data = geo.apply(lambda x : x.length)\n",
    "length_data = np.asarray(length_data)\n",
    "length_data = np.reshape(length_data, (-1, 1))\n",
    "z_length_data = (length_data - np.mean(length_data, axis=0))/np.std(length_data, axis=0) \n",
    "\n",
    "\n",
    "# Get area for submission\n",
    "area_data_test = geo_test.apply(lambda x : x.area)\n",
    "area_data_test = np.asarray(area_data_test)\n",
    "area_data_test = np.reshape(area_data_test, (-1, 1))\n",
    "n_area_data_test = (area_data_test - np.mean(area_data, axis=0))/np.std(area_data, axis=0) \n",
    "\n",
    "length_data_test = geo_test.apply(lambda x : x.length)\n",
    "length_data_test = np.asarray(length_data_test)\n",
    "length_data_test = np.reshape(length_data_test, (-1, 1))\n",
    "z_length_data_test = (length_data_test - np.mean(length_data, axis=0))/np.std(length_data, axis=0) \n",
    "print(n_area_data_test.shape)\n",
    "# Get shape of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo = geo.apply(coord_lister)\n",
    "\n",
    "# print(area_data, length_data)\n",
    "# array = np.asarray(geo)\n",
    "\n",
    "# array = np.array([e for e in array])\n",
    "# x = array[:, ::2][:,:-1]\n",
    "# y = array[:, 1::2][:, :-1]\n",
    "\n",
    "# x_min = np.min(x, axis=0)\n",
    "# y_min = np.min(y, axis=0)\n",
    "\n",
    "# x_max = np.max(x, axis=0)\n",
    "# y_max = np.max(y, axis=0)\n",
    "\n",
    "# x_normalized_data = (x - x_min)/(x_max-x_min)\n",
    "# y_normalized_data = (y - y_min)/(y_max-y_min)\n",
    "\n",
    "# x_mean = np.mean(x, axis=1)\n",
    "# y_mean = np.mean(y, axis = 1)\n",
    "\n",
    "\n",
    "# X_mean = np.tile(x_mean, (4,1)).T\n",
    "# Y_mean = np.tile(y_mean, (4,1)).T\n",
    "\n",
    "# x_data = x - X_mean\n",
    "# y_data = y - Y_mean\n",
    "\n",
    "\n",
    "# x_mean = np.reshape(x_mean,(-1, 1))\n",
    "# y_mean = np.reshape(y_mean,(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Geotypes processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all types\n",
    "geotypes = set()\n",
    "\n",
    "geography_types_test = test_df[\"geography_type\"]\n",
    "geography_types_array_test = np.asarray(test_df[\"geography_type\"])\n",
    "\n",
    "geography_types = clean_df[\"geography_type\"]\n",
    "geography_types_array = np.asarray(clean_df[\"geography_type\"])\n",
    "\n",
    "def count(x) :\n",
    "    l = x.split(',')\n",
    "    for m in l :\n",
    "        if m == 'A' or m =='N' :\n",
    "            break\n",
    "        geotypes.add(m)\n",
    "    return x\n",
    "count = np.vectorize(count)\n",
    "count(geography_types_array)\n",
    "\n",
    "geotypes = list(geotypes)\n",
    "size = len(geotypes)\n",
    "\n",
    "geotypes_data = np.ndarray((geography_types_array.shape[0], size))\n",
    "geotypes_data_test = np.ndarray((geography_types_array_test.shape[0], size))\n",
    "\n",
    "# Compute for training\n",
    "for i,t in enumerate(geography_types_array) :\n",
    "    for m in t.split(',') :\n",
    "        if m == 'A' or m =='N' :\n",
    "            break\n",
    "        geotypes_data[i,geotypes.index(m)] = 1\n",
    "\n",
    "# Compute for submission\n",
    "for i,t in enumerate(geography_types_array_test) :\n",
    "    for m in t.split(',') :\n",
    "        if m == 'A' or m =='N' :\n",
    "            break\n",
    "        geotypes_data_test[i,geotypes.index(m)] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Urbantypes processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Get all types\n",
    "urbantypes = set()\n",
    "\n",
    "urban_type_test = test_df[\"urban_type\"]\n",
    "urban_types_array_test = np.asarray(test_df[\"urban_type\"])\n",
    "\n",
    "urban_types = clean_df[\"urban_type\"]\n",
    "urban_types_array = np.asarray(clean_df[\"urban_type\"])\n",
    "\n",
    "def count(x) :\n",
    "    l = x.split(',')\n",
    "    for m in l :\n",
    "        if m != 'A' and m != 'N' :\n",
    "            urbantypes.add(m)\n",
    "    return x\n",
    "\n",
    "count = np.vectorize(count)\n",
    "count(urban_types_array)\n",
    "urbantypes = list(urbantypes)\n",
    "size = len(urbantypes)\n",
    "\n",
    "# Compute for training set\n",
    "\n",
    "urbantypes_data = np.ndarray((urban_types_array.shape[0], size))\n",
    "for i,t in enumerate(urban_types_array) :\n",
    "    for m in t.split(',') :\n",
    "        if m == 'A' or m == 'N' :\n",
    "            break\n",
    "        urbantypes_data[i,urbantypes.index(m)] = 1\n",
    "\n",
    "# Compute for submission set\n",
    "\n",
    "urbantypes_data_test = np.ndarray((urban_types_array_test.shape[0], size))\n",
    "for i,t in enumerate(urban_types_array_test) :\n",
    "    for m in t.split(',') :\n",
    "        if m == 'A' or m == 'N' :\n",
    "            break\n",
    "        urbantypes_data_test[i,urbantypes.index(m)] = 1\n",
    "\n",
    "print(urbantypes_data[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Status preprocessing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "columns = [col for col in clean_df if col.startswith(\"change_status\")]\n",
    "status = clean_df[columns]\n",
    "status_test = test_df[columns]\n",
    "\n",
    "status = np.asarray(status)\n",
    "status_test = np.asarray(status_test)\n",
    "\n",
    "# Fit OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(status)\n",
    "\n",
    "# Encode training\n",
    "status_data = encoder.transform(status).toarray()\n",
    "\n",
    "# Encode submission\n",
    "status_data_test = encoder.transform(status_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Date processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "columns = [col for col in clean_df if col.startswith(\"date\")]\n",
    "dates = clean_df[columns]\n",
    "dates_test = test_df[columns]\n",
    "\n",
    "\n",
    "for i in range(5) :\n",
    "    dates[[f'day{i}',f'month{i}',f'year{i}']] = dates[f'date{i}'].str.split('-', expand=True).astype(int)\n",
    "    dates_test[[f'day{i}',f'month{i}',f'year{i}']] = dates_test[f'date{i}'].str.split('-', expand=True).astype(int)\n",
    "\n",
    "c_days = [col for col in dates if col.startswith(\"day\")]\n",
    "c_months = [col for col in dates if col.startswith(\"month\")]\n",
    "c_year = [col for col in dates if col.startswith(\"year\")]\n",
    "\n",
    "time = pd.concat([dates[c_days],dates[c_months], dates[c_year]], axis=1)\n",
    "time = pd.concat([dates_test[c_days],dates_test[c_months], dates_test[c_year]], axis=1)\n",
    "\n",
    "def time(row, j) :\n",
    "    return row[f'year{j}'] * 365 + row[f'month{j}'] * 30 + row[f'day{j}']  * 1\n",
    "\n",
    "for i in range(5) :\n",
    "    dates[f'time{i}'] = dates.apply(time, axis=1, j=i).astype(int)\n",
    "    dates_test[f'time{i}'] = dates_test.apply(time, axis=1, j=i).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68249661 0.2721843  0.21245003 0.27010622]\n",
      " [0.68249661 0.2721843  0.21245003 0.27010622]\n",
      " [0.68249661 0.2721843  0.21245003 0.27010622]\n",
      " ...\n",
      " [0.19810041 0.3447099  0.24386065 0.30197269]\n",
      " [0.19810041 0.3447099  0.24386065 0.30197269]\n",
      " [0.19810041 0.3447099  0.24386065 0.30197269]] [[0.63975577 0.42491468 0.29297544 0.26783005]\n",
      " [0.63975577 0.42491468 0.29297544 0.26783005]\n",
      " [0.63975577 0.42491468 0.29297544 0.26783005]\n",
      " ...\n",
      " [0.30054274 0.61348123 0.54368932 0.2875569 ]\n",
      " [0.30054274 0.61348123 0.54368932 0.2875569 ]\n",
      " [0.30054274 0.61348123 0.54368932 0.2875569 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "time_array = np.stack(dates[[f'time{i}' for i in range(5)]].astype(int).apply(np.sort, axis=1).to_numpy())\n",
    "time_array_test = np.stack(dates_test[[f'time{i}' for i in range(5)]].astype(int).apply(np.sort, axis=1).to_numpy())\n",
    "\n",
    "elapsed = np.ndarray((time_array.shape[0],4))\n",
    "elapsed_test = np.ndarray((time_array_test.shape[0],4))\n",
    "\n",
    "for i in range(4) :\n",
    "    elapsed[:,i] = time_array[:,i+1] - time_array[:,i]\n",
    "    elapsed_test[:,i] = time_array_test[:,i+1] - time_array_test[:,i]\n",
    "\n",
    "z_elapsed = (elapsed - np.min(elapsed, axis=0)) / (np.max(elapsed, axis=0)- np.min(elapsed, axis=0))\n",
    "z_elapsed_test = (elapsed_test - np.min(elapsed, axis=0)) / (np.max(elapsed, axis=0) - np.min(elapsed, axis=0))\n",
    "print(z_elapsed, z_elapsed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Color data processing <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 30)\n"
     ]
    }
   ],
   "source": [
    "# Selects every column starting with \"img\"\n",
    "# We could have separated std deviation and mean ?\n",
    "cols = [col for col in clean_df if \"img\" in col]\n",
    "\n",
    "# Build an array from the data\n",
    "color = np.asarray(clean_df[cols])\n",
    "mean = np.mean(color, axis=0)\n",
    "std = np.std(color, axis=0)\n",
    "ma = np.max(color, axis=0)\n",
    "mi = np.min(color, axis=0)\n",
    "\n",
    "color_test = test_df[cols]\n",
    "mean_test = np.mean(color_test, axis=0)\n",
    "std_test = np.std(color_test, axis=0)\n",
    "ma_test = np.max(color_test, axis=0)\n",
    "mi_test = np.min(color_test, axis=0)\n",
    "\n",
    "minimax_color = (color-mi)/(ma-mi)\n",
    "z_color = (color-mean)/std\n",
    "z_color_test = (color_test - mean)/std\n",
    "print(color.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color='#0590C0'> Training <font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 103)\n",
      "(296146,)\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Create training set\n",
    "labels = [\"Demolition\", \"Road\", \"Residential\", \"Commercial\", \"Industrial\", \"Mega Projects\"]\n",
    "X = np.concatenate((n_area_data, geotypes_data, urbantypes_data, status_data, z_color, z_length_data), axis=1)\n",
    "#X_new = SelectKBest(f_classif, k=30).fit_transform(X, y)\n",
    "X_new = X\n",
    "print(X_new.shape)\n",
    "\n",
    "data_labels = clean_df[\"change_type\"]\n",
    "data_labels = data_labels.apply(lambda x : labels.index(x))\n",
    "\n",
    "y = np.asarray(data_labels)\n",
    "print(y.shape)\n",
    "\n",
    "# Vector data set\n",
    "Y_vector = utils.to_categorical(y,num_classes=6)\n",
    "X_train, X_test, Y_vector_train, Y_vector_test = train_test_split(X_new, Y_vector, test_size=0.1, random_state=42)\n",
    "\n",
    "# Class data set\n",
    "Y_class = np.asarray(y)\n",
    "\n",
    "X_train, X_test, Y_class_train, Y_class_test = train_test_split(X_new, Y_class, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, Y_class_train);\n",
    "clf.score(X_train[:1000,:], Y_class_train[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.665"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test[:200], Y_class_test[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import models, layers\n",
    "\n",
    "dataset_size = X_train.shape[0]\n",
    "features = X_train.shape[1]\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(2000, activation='relu'))\n",
    "model.add(layers.Dense(2000, activation='relu'))\n",
    "model.add(layers.Dense(2000, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.build((dataset_size,features))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_vector_train,batch_size=30, validation_split=0.2, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate neural network\n",
    "model.evaluate(X_test, Y_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.634\n",
      "0.816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X_train, Y_class_train)\n",
    "print(neigh.score(X_test[:2000], Y_class_test[:2000]))\n",
    "print(neigh.score(X_train[:2000], Y_class_train[:2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.722741853790309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_features=0.4, n_estimators=100)\n",
    "clf.fit(X_train, Y_class_train)\n",
    "print(clf.score(X_train[:5000],Y_class_train[:5000]))\n",
    "print(clf.score(X_test,Y_class_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = np.concatenate((area_data_test, geotypes_data_test, urbantypes_data_test, status_data_test, color_test, length_data_test), axis=1)\n",
    "Y_submission = clf.predict(X_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120526,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Y_submission.shape)\n",
    "s = pd.DataFrame(Y_submission, columns=['change_type'])\n",
    "s.index.name = \"Id\"\n",
    "s.to_csv('no_elapsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "data = pandas.read_csv(\"data/Sorted_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns= ['index','urban_type', 'geography_type', 'change_type', 'date0', 'date1', 'date2', 'date3', 'date4', 'geometry'] , axis=1)\n",
    "Y = data['change_type']\n",
    "\n",
    "X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_vector_train, Y_vector_test = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "clf.fit(X_train, Y_vector_train)\n",
    "\n",
    "clf.score(X_test[:1000], Y_vector_test[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.931"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train[:1000], Y_vector_train[:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
